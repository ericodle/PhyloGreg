import torch
from torch.nn import Embedding
import numpy as np
from Bio import SeqIO

# Define parameters
max_seq_len = 100  # Maximum sequence length
embedding_dim = 50  # Embedding dimension
batch_size = 32  # Batch size
learning_rate = 0.001  # Learning rate
epochs = 10  # Number of epochs
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Read FASTA file
sequences = []
for record in SeqIO.parse("sequences.fasta", "fasta"):
    sequences.append(str(record.seq))

# Tokenize sequences
tokenized_seqs = [[ord(char) for char in seq] for seq in sequences]
padded_seqs = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(seq[:max_seq_len]).to(device) for seq in tokenized_seqs],
                                               batch_first=True)

# Define embedding layer
embedding_layer = Embedding(256, embedding_dim).to(device)

# Define model
class EmbeddingModel(torch.nn.Module):
    def __init__(self, embedding_layer):
        super().__init__()
        self.embedding = embedding_layer

    def forward(self, x):
        embedded = self.embedding(x)
        return embedded

model = EmbeddingModel(embedding_layer).to(device)

# Define loss function and optimizer
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train model
for epoch in range(epochs):
    epoch_loss = 0
    for i in range(0, len(padded_seqs), batch_size):
        batch = padded_seqs[i:i+batch_size]
        optimizer.zero_grad()
        embeddings = model(batch)
        loss = criterion(embeddings.sum(dim=1), torch.zeros(batch_size, embedding_dim).to(device))
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f"Epoch {epoch+1}: Loss {epoch_loss:.5f}")

# Get embeddings
embeddings = model(padded_seqs)

# Calculate distances between embeddings
distances = np.zeros((len(sequences), len(sequences)))
for i in range(len(sequences)):
    for j in range(i+1, len(sequences)):
        distances[i][j] = np.linalg.norm(embeddings[i].detach().cpu().numpy() - embeddings[j].detach().cpu().numpy())
        distances[j][i] = distances[i][j]

# Plot distances
import matplotlib.pyplot as plt
plt.imshow(distances, cmap='viridis')
plt.colorbar()
plt.show()
