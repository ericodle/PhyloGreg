import torch
import torch.nn as nn
import torch.optim
from torch.utils.data import DataLoader
from Bio import SeqIO
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.utils.rnn as rnn_utils
import torch
from torch.utils.data import Dataset
import json
import torch

# Define the IUPAC codes for nucleotide ambiguity
NUCLEOTIDE_CODES = {
    "A": [1, 0, 0, 0],
    "C": [0, 1, 0, 0],
    "G": [0, 0, 1, 0],
    "T": [0, 0, 0, 1],
    "U": [0, 0, 0, 1],
    "R": [0.5, 0, 0.5, 0],
    "Y": [0, 0.5, 0, 0.5],
    "S": [0, 0.5, 0.5, 0],
    "W": [0.5, 0, 0, 0.5],
    "K": [0, 0, 0.5, 0.5],
    "M": [0.5, 0.5, 0, 0],
    "B": [0, 1/3, 1/3, 1/3],
    "D": [1/3, 0, 1/3, 1/3],
    "H": [1/3, 1/3, 0, 1/3],
    "V": [1/3, 1/3, 1/3, 0],
    "N": [0.25, 0.25, 0.25, 0.25],
    "-": [0, 0, 0, 0]  # for gaps
}

# define a collate function to pad sequences
def collate_fn(batch):
    sequences, lengths = zip(*batch)
    sequences_padded = rnn_utils.pad_sequence(sequences, batch_first=True)
    return sequences_padded, torch.tensor(lengths)

# Define a function to convert a sequence string to a tensor of embeddings
def sequence_to_tensor(sequence):
    embedding = []
    for base in sequence.upper():
        if base in NUCLEOTIDE_CODES:
            embedding.append(NUCLEOTIDE_CODES[base])
        else:
            raise ValueError(f"Invalid nucleotide {base}")
    return torch.tensor(embedding)


def read_fasta(file_path):
        sequences = []
        labels = []
        with open(file_path, 'r') as f:
            sequence = ''
            for line in f:
                if line.startswith('>'):
                    label = line.strip()[1:]
                    labels.append(label)
                    if sequence:
                        sequences.append(sequence)
                    sequence = ''
                else:
                    sequence += line.strip()
            sequences.append(sequence)
        return sequences, labels

# Define the vocabulary and mapping
vocab = "abcdefghijklmnopqrstuvwxyz"
char_to_idx = {char: i for i, char in enumerate(vocab)}


def save_data_as_json(sequences, labels, filename):
    data = {
        "sequences": sequences.tolist(),
        "labels": labels.tolist()
    }
    with open(filename, "w") as f:
        json.dump(data, f)


sequences, labels = read_fasta('/home/eo/Desktop/dendro/euk_silva.fasta')
print(len(sequences))
print(len(labels))

embeddings = []
embedding_labels = []

for sequence in sequences:
    embeddings.append(sequence_to_tensor(sequence))

print("sequences tokenized and converted to tensor list")

for label in labels:
    embedding_labels.append(torch.LongTensor([char_to_idx[char] for char in label]))

print("labels converted to tensor list")


# Assuming `sequences` and `labels` are PyTorch tensors containing your data
save_data_as_json(embeddings, embedding_labels, "/home/eo/Desktop/dendro/data.json")

print("tokenized sequences and corresponding labels saved as JSON file")
    
