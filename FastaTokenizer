# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')


!pip install biopython



import torch
import torch.nn as nn

class DNEmbedding(nn.Module):
    def __init__(self, input_dim, embedding_dim):
        super(DNEmbedding, self).__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
    
    def forward(self, fasta_file):
        """
        fasta_file: path to a FASTA file containing DNA sequences
        """
        # read the sequences from the FASTA file
        seqs = []
        with open(fasta_file) as f:
            seq = ""
            for line in f:
                if line.startswith(">"):
                    if seq:
                        seqs.append(seq)
                    seq = ""
                else:
                    seq += line.strip()
            seqs.append(seq)
        
        # convert nucleotide letters to integer indices
        # A=0, C=1, G=2, T=3, N=4
        indices = []
        for seq in seqs:
            seq_indices = []
            for letter in seq:
                if letter == 'A':
                    seq_indices.append(0)
                elif letter == 'C':
                    seq_indices.append(1)
                elif letter == 'G':
                    seq_indices.append(2)
                elif letter == 'U':
                    seq_indices.append(3)
                else:
                    seq_indices.append(4)
            indices.append(seq_indices)

        # pad the sequences to the same length
        max_len = max(len(seq) for seq in indices)
        padded_indices = [seq + [4]*(max_len-len(seq)) for seq in indices]

        # convert the indices to a LongTensor and move to GPU
        input_seqs = torch.LongTensor(padded_indices).cuda()

        # pass the input sequences through the embedding layer and move to CPU
        embedded = self.embedding(input_seqs).cpu()
        return embedded
        
        
        # check if GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

# create an embedding layer with input dimension 5 (for ACGTN) and embedding dimension 16
embedding = DNEmbedding(5, 16).to(device)

# input a FASTA file on GPU
fasta_file = "example.fasta"
embedded_seqs = embedding(fasta_file)  # shape: (num_seqs, max_len, embedding_dim)
